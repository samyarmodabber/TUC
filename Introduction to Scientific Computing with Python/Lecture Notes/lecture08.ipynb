{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IyuBmjJqKxBc",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Automatic Differentiation\n",
    "\n",
    "Parts of this is inspired by [this tutorial](https://github.com/DataScienceUB/DeepLearningMaster2019/blob/master/2.%20Automatic%20Differentiation.ipynb)\n",
    "\n",
    "In many applications it is crucial to compute derivatives such as machine learning, optimization problems, and so on. Often the most basis idea is to approximate the gradient of a function by computing a finite difference approximation\n",
    "$$\n",
    "\\frac{f(x_1+h,x_2,\\ldots,x_n)-f(x_1,x_2,\\ldots,x_n)}{h}\n",
    "$$\n",
    "or numerically a little better using a centered difference\n",
    "$$\n",
    "\\frac{f(x_1+h,x_2,\\ldots,x_n)-f(x_1-h,x_2,\\ldots,x_n)}{2h}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "def diff_f(x,h):\n",
    "    fvec = np.exp(x)\n",
    "    y = np.abs((np.exp(x+h)-np.exp(x-h))/(2*h)-fvec)/np.abs(fvec)\n",
    "    return y\n",
    "\n",
    "def diff2_f(x,h):\n",
    "    fvec = np.exp(x)\n",
    "    y = np.abs((np.exp(x+h)-np.exp(x))/(h)-fvec)/np.abs(fvec)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1 = np.logspace(-10, 1, 100, endpoint=True)# range of h parameter\n",
    "y = diff_f(5,x1)\n",
    "y2 = diff2_f(5,x1)\n",
    "plt.loglog(x1, y,color='red')\n",
    "plt.loglog(x1, y2,color='blue')\n",
    "y = diff_f(2,x1)\n",
    "y2 = diff2_f(2,x1)\n",
    "plt.loglog(x1, y,color='red',linestyle='--')\n",
    "plt.loglog(x1, y2,color='blue',linestyle='--')\n",
    "y = diff_f(150,x1)\n",
    "y2 = diff2_f(150,x1)\n",
    "plt.loglog(x1, y,color='red',linestyle='-.')\n",
    "plt.loglog(x1, y2,color='blue',linestyle='-.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This method can be quite slow and is prone to rounding errors. The following Figure illustrates the main idea behind back propagation which is based on automatic differentiation.\n",
    "![](https://miro.medium.com/max/771/1*DcLWqOojI1b9jzQaLibUkQ.png)\n",
    "\n",
    "> The **backpropagation** algorithm was originally introduced in the 1970s, but its importance wasn't fully appreciated until a famous 1986 paper by David Rumelhart, Geoffrey Hinton, and Ronald Williams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> **Backpropagation** is the key algorithm that makes training deep models computationally tractable. For modern neural networks, it can make training with gradient descent as much as ten million times faster, relative to a naive implementation. That’s the difference between a model taking a week to train and taking 200,000 years. (Christopher Olah, 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have seen that in order to optimize our models we need to compute the derivative of the loss function with respect to all model paramaters. \n",
    "\n",
    "The computation of derivatives in computer models is addressed by four main methods: \n",
    "\n",
    "+ Manually working out derivatives and coding the result (as in the original paper describing backpropagation); \n",
    "\n",
    "![alt text](https://github.com/DataScienceUB/DeepLearningMaster2019/blob/master/images/back.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "+ Numerical differentiation (using finite difference approximations); \n",
    "+ Symbolic differentiation (using expression manipulation in software, such as `Sympy`); \n",
    "+ and Automatic differentiation (AD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Automatic differentiation** (AD) works by systematically applying the **chain rule** of differential calculus at the elementary operator level.\n",
    "\n",
    "Let $ y = f(g(x)) $ be our target function. In its basic form, the chain rule states:\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial x} = \\frac{\\partial f}{\\partial g} \\frac{\\partial g}{\\partial x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "or, if there are more than one variable $g_i$ in-between $y$ and $x$ (f.e. if $f$ is a two dimensional function such as $f(g_1(x), g_2(x))$), then:\n",
    "\n",
    "$$ \\frac{\\partial f}{\\partial x} = \\sum_i \\frac{\\partial f}{\\partial g_i} \\frac{\\partial g_i}{\\partial x} $$\n",
    "\n",
    "> See http://tutorial.math.lamar.edu/Classes/CalcIII/ChainRule.aspx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, let's see how AD allows the accurate evaluation of derivatives at machine precision, with only a small constant factor of overhead.\n",
    "\n",
    "In its most basic description, AD relies on the fact that all numerical computations\n",
    "are ultimately compositions of a finite set of elementary operations for which derivatives are known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRNAJqxYKxBf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, let's consider the computation of the derivative of the sigmoid\n",
    "\n",
    "$$\n",
    "    f(x) = \\frac{1}{1 + e^{- ({w}^T \\cdot  x + b)}} \n",
    "$$\n",
    "\n",
    "\n",
    "First, let's write how to evaluate $f(x)$ via a sequence of primitive operations (here for scalar $w$ and $x$):\n",
    "\n",
    "\n",
    "```python\n",
    "x = ?\n",
    "f1 = w * x\n",
    "f2 = f1 + b\n",
    "f3 = -f2\n",
    "f4 = 2.718281828459 ** f3\n",
    "f5 = 1.0 + f4\n",
    "f = 1.0/f5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRNAJqxYKxBf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The question mark indicates that $x$ is a value that must be provided. \n",
    "\n",
    "This *program* can compute the value of $x$ and also **populate program variables**, which means to give the defined variables a value. \n",
    "\n",
    "We can evaluate $\\frac{\\partial f}{\\partial x}$ at some $x$ by using the chain rule. This is called **forward-mode differentiation**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRNAJqxYKxBf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In principle we follow\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x}=\\frac{\\partial y}{\\partial w_{n-1}}\\frac{\\partial w_{n-1}}{\\partial x}=\\frac{\\partial y}{\\partial w_{n-1}}\\left(\\frac{\\partial w_{n-1}}{\\partial w_{n-2}}\\frac{\\partial w_{n-2}}{\\partial x}\\right)=\\ldots\n",
    "$$\n",
    "\n",
    "In our case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "xuWlp5RzKxBg",
    "outputId": "7be084e4-7745-46b8-8ac4-0f3a2768a0b0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def f(x,w,b):\n",
    "    f1 = w * x\n",
    "    f2 = f1 + b\n",
    "    f3 = -f2\n",
    "    f4 = 2.718281828459 ** f3\n",
    "    f5 = 1.0 + f4\n",
    "    return 1.0/f5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfdx_forward(x, w, b):\n",
    "    f1 = w * x\n",
    "    p1 = w                          # p1 = df1/dx\n",
    "    f2 = f1 + b\n",
    "    p2 = 1.0*p1                     # p2 = df2/df1*p1 \n",
    "    f3 = -f2\n",
    "    p3 = -1.0*p2                    # p3 = df3/df2*p2\n",
    "    f4 = 2.718281828459 ** f3\n",
    "    p4 = 2.718281828459 ** f3 *p3    # p4 = df4/df3*p3\n",
    "    f5 = 1.0 + f4\n",
    "    p5 = 1.0*p4                     # p5 = df5/df4*p4\n",
    "    f6 = 1.0/f5\n",
    "    dfx = -1.0 / f5 ** 2.0*p5        # df/dx = df6/df5*p5\n",
    "    return f6, dfx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "xuWlp5RzKxBg",
    "outputId": "7be084e4-7745-46b8-8ac4-0f3a2768a0b0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the function at (3, 2, 1):  0.9990889488055992\n",
      "df/dx Derivative (fin diff) at (3, 2, 1):  0.0018204406870836465\n",
      "df/dx Derivative (aut diff) at (3, 2, 1):  0.0018204423602438654\n",
      "The difference between both is given as: 1.6731602188804762e-09\n"
     ]
    }
   ],
   "source": [
    "h = 0.000001;\n",
    "der = (f(3+h, 2, 1) - f(3, 2, 1))/h\n",
    "\n",
    "print(\"Value of the function at (3, 2, 1): \",f(3, 2, 1))\n",
    "print(\"df/dx Derivative (fin diff) at (3, 2, 1): \",der)\n",
    "print(\"df/dx Derivative (aut diff) at (3, 2, 1): \",dfdx_forward(3, 2, 1)[1])\n",
    "print(\"The difference between both is given as:\",dfdx_forward(3, 2, 1)[1]-der)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We consider the function \n",
    "$$\n",
    "g(x_1,x_2)=\\sin(x_1)+x_1x_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def g(x1,x2):\n",
    "    f1 = x1\n",
    "    f2 = x2\n",
    "    f3 = f1*f2\n",
    "    f4 = math.sin(f1)\n",
    "    f5 = f4+f3\n",
    "    \n",
    "    return f5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def dgdx1_forward(x1,x2):\n",
    "    f1 = x1\n",
    "    p1 = 1  # p1=df1/dx1\n",
    "    f2 = x2\n",
    "    p2 = 0  # p2=df2/dx1\n",
    "    f3 = f1*f2\n",
    "    p3 = p1*f2+p2*f1 # p3 = f1'f2+f1f2'\n",
    "    f4 = math.sin(f1)\n",
    "    p4 = math.cos(f1)*p1 # d\n",
    "    f5 = f4+f3\n",
    "    p5 = p4+p3;\n",
    "    return p5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def dgdx1_forward2(x1,x2):\n",
    "    f1 = x1\n",
    "    p1 = 0  # p1=df1/dx1\n",
    "    f2 = x2\n",
    "    p2 = 1  # p2=df2/dx1\n",
    "    f3 = f1*f2\n",
    "    p3 = p1*f2+p2*f1 # p3 = f1'f2+f1f2'\n",
    "    f4 = math.sin(f1)\n",
    "    p4 = math.cos(f1)*p1 # d\n",
    "    f5 = f4+f3\n",
    "    p5 = p4+p3;\n",
    "    return p5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the function at (3, 2, 1):  3.1411200080598674\n",
      "dg/dx1 Derivative (fin diff) at (3, 2, 1):  0.010007497053265979\n",
      "dg/dx1 Derivative (aut diff) at (3, 2, 1):  0.010007503399554585\n",
      "The difference between both is given as: 6.346288605740824e-09\n"
     ]
    }
   ],
   "source": [
    "h = 0.0000001;\n",
    "der = (g(3+h, 1) - g(3,1))/h\n",
    "print(\"Value of the function at (3, 2, 1): \",g(3, 1))\n",
    "print(\"dg/dx1 Derivative (fin diff) at (3, 2, 1): \",der)\n",
    "print(\"dg/dx1 Derivative (aut diff) at (3, 2, 1): \",dgdx1_forward(3, 1))\n",
    "print(\"The difference between both is given as:\",dgdx1_forward(3, 1)-der) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the function at (3, 2, 1):  3.1411200080598674\n",
      "dg/dx1 Derivative (fin diff) at (3, 2, 1):  2.9999999995311555\n",
      "dg/dx1 Derivative (aut diff) at (3, 2, 1):  3.0\n",
      "The difference between both is given as: 4.688445187639445e-10\n"
     ]
    }
   ],
   "source": [
    "h = 0.0000001;\n",
    "der = (g(3, 1+h) - g(3,1))/h\n",
    "print(\"Value of the function at (3, 2, 1): \",g(3, 1))\n",
    "print(\"dg/dx1 Derivative (fin diff) at (3, 2, 1): \",der)\n",
    "print(\"dg/dx1 Derivative (aut diff) at (3, 2, 1): \",dgdx1_forward2(3, 1))\n",
    "print(\"The difference between both is given as:\",dgdx1_forward2(3, 1)-der) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2cdOSRU0KxBn",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is interesting to note that this *program* can be automatically derived  if we have access to **subroutines implementing the derivatives of primitive functions** (such as $\\exp{(x)}$ or $1/x$) and all intermediate variables are computed in the right order. \n",
    "\n",
    "It is also interesting to note that AD allows the accurate evaluation of derivatives at **machine precision**, with only a small constant factor of overhead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-2EJLmpKxBt",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The above can be viewed as differentiation of each equation with respect to some yet-to-be-given variable $x$ via the chain schematically given as\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x}=\\frac{\\partial y}{\\partial w_{n-1}}\\frac{\\partial w_{n-1}}{\\partial x}=\\frac{\\partial y}{\\partial w_{n-1}}\\left(\\frac{\\partial w_{n-1}}{\\partial w_{n-2}}\\frac{\\partial w_{n-2}}{\\partial x}\\right)=\\ldots\n",
    "$$\n",
    "where we can substitute $x=x_1$ or $x=x_2$ to obtain the desired derivative. This forward differentiation is efficient for functions $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ with $n \\ll m$ (only $O(n)$ sweeps are necessary) as we need to call the function for $n$ different values of $x$. What if we want to avoid this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-2EJLmpKxBt",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Alternatively, we can consider a backward version of the differentiation by proceeding as follows\n",
    "\\begin{align}\n",
    "\\frac{\\partial y}{\\partial x}=\\frac{\\partial y}{\\partial w_1}\\frac{\\partial w_1}{\\partial x}=\\left(\\frac{\\partial y}{\\partial w_2}\\frac{\\partial w_2}{\\partial w_1}\\right)\\frac{\\partial w_1}{\\partial x}=\\ldots\n",
    "\\end{align}\n",
    "using the function\n",
    "\n",
    "```python\n",
    "    w1 = x1\n",
    "    w2 = x2\n",
    "    w3 = w1*w2\n",
    "    w4 = math.sin(w1)\n",
    "    w5 = w4+w3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let’s take a second look at the chain rule we used to derive forward-mode AD for a generic variable $t$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial w}{\\partial t}\n",
    "=\n",
    "\\sum_i\n",
    "(\n",
    "\\frac{\\partial y}{\\partial w_i}\n",
    "\\frac{\\partial w_i}{\\partial t}\n",
    ")\n",
    "=\n",
    "\\frac{\\partial y}{\\partial w_1}\n",
    "\\frac{\\partial w_1}{\\partial t}\n",
    "+\n",
    "\\frac{\\partial y}{\\partial w_2}\n",
    "\\frac{\\partial w_2}{\\partial t}\n",
    "+\\ldots\n",
    "$$\n",
    "\n",
    "To calculate the gradient using forward-mode AD, we had to perform two substitutions: one with $t=x_1$\n",
    "and another with $t=x_2$.\n",
    "\n",
    "This means we have to **run the code twice.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However, the chain rule is symmetric: it doesn’t care what’s in the “numerator” or the “denominator”. So let’s rewrite the chain rule but turn the derivatives upside down:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial w_j}\n",
    "=\n",
    "\\sum_i\n",
    "(\n",
    "\\frac{\\partial w_i}{\\partial w_j}\n",
    "\\frac{\\partial z}{\\partial w_i}\n",
    ")\n",
    "=\n",
    "\\frac{\\partial w_1}{\\partial w_j}\n",
    "\\frac{\\partial z}{\\partial w_1}\n",
    "+\n",
    "\\frac{\\partial w_2}{\\partial w_j}\n",
    "\\frac{\\partial z}{\\partial w_2}\n",
    "+\\ldots\n",
    "$$\n",
    "\n",
    "In this case we obtain the derivative with respect to a variable $w_j$ for some $j$ where $z$ is the output variable\n",
    "$$\n",
    "z=h(x_1,x_2)=x_1x_2+\\sin(x_1)\n",
    "$$\n",
    "and here we assume this is only one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-2EJLmpKxBt",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Reverse mode automatic differentiation proceeds by computing for $z=w_5$ that\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial z}=\\frac{\\partial z}{\\partial w_5}=1.\n",
    "$$\n",
    "Now we know that $w_5=w_4+w_3$ and hence we get\n",
    "$$\n",
    "\\frac{\\partial w_5}{\\partial w_4}=1,\\quad\\frac{\\partial w_5}{\\partial w_3}=1.\n",
    "$$\n",
    "Now using the chain rule we get\n",
    "\\begin{align}\n",
    "\\frac{\\partial z}{\\partial w_3}&=\\frac{\\partial z}{\\partial w_5}\\frac{\\partial w_5}{\\partial w_3}=1\\\\\n",
    "\\frac{\\partial z}{\\partial w_4}&=\\frac{\\partial z}{\\partial w_5}\\frac{\\partial w_5}{\\partial w_4}=1.\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-2EJLmpKxBt",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, we remember that $w_3=w_2w_1$ and get $\\frac{\\partial w_3}{\\partial w_2}=w_1$ and  $\\frac{\\partial w_3}{\\partial w_1}=w_2$. This can be combined to obtain the following derivative\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial w_2}=\\frac{\\partial z}{\\partial w_3}\\frac{\\partial w_3}{\\partial w_2}=1\\cdot w_1=x_1.\n",
    "$$\n",
    "Also, $w_1$ contributes to $z$ and we get\n",
    "\\begin{align}\n",
    "\\frac{\\partial z}{\\partial w_1}&=\\frac{\\partial z}{\\partial w_3}\\frac{\\partial w_3}{\\partial w_1}+\\frac{\\partial z}{\\partial w_4}\\frac{\\partial w_4}{\\partial w_1}\\\\\n",
    "&=w_2+\\mathrm{cos}(w_1)\\\\\n",
    "&=x_2+\\mathrm{cos}(x_1)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-2EJLmpKxBt",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is called *reverse-mode differentiation* or *backwards propagation*. Reverse pass starts at the end (i.e. $\\frac{\\partial z}{\\partial z} = 1$) and propagates backward to all dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "af4RMYaIKxBu",
    "outputId": "bd858897-e072-4c76-d0b4-55aa98e63715",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def dgdx_backward(x1, x2):\n",
    "    import numpy as np\n",
    "    w1 = x1\n",
    "    w2 = x2\n",
    "    w3 = w1*w2\n",
    "    w4 = math.sin(w1)\n",
    "    w5 = w4+w3\n",
    "    z = w5\n",
    "    \n",
    "    dz = 1             # pf = df/df\n",
    "    d5d4 = dz*1.0         # p5 = pf * df/df5 \n",
    "    d5d3 = dz*1.0                      # p4 = p5 * df5/df4\n",
    "    d3d2 = w1       # p3 = p4 * df4/df3\n",
    "    d3d1 = w2\n",
    "    d4d1 = math.cos(w1)    # p2 = p3 * df3/df2\n",
    "    dzd1 = d5d3*d3d1+d5d4*d4d1                      # p1 = p2 * df2/df1\n",
    "    dzd2 = d5d3*d3d2                      # df/dx = p1 * df1/dx \n",
    "    return dzd1, dzd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "af4RMYaIKxBu",
    "outputId": "bd858897-e072-4c76-d0b4-55aa98e63715",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finite difference approximation: 2.540302261877514 0.999999993922529\n",
      "dg/dx Derivative (aut diff) at (3, 2, 1):  (2.5403023058681398, 1.0)\n"
     ]
    }
   ],
   "source": [
    "h = 0.0000001;\n",
    "der1 = (g(1+h, 2) - g(1,2))/h\n",
    "der2 = (g(1, 2+h) - g(1,2))/h\n",
    "\n",
    "print(\"Finite difference approximation:\",der1,der2)\n",
    "print(\"dg/dx Derivative (aut diff) at (3, 2, 1): \",\n",
    "      dgdx_backward(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f0I-yLHWKxB6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In practice, reverse-mode differentiation is a two-stage process. In the first stage the original function code is run forward, populating the $w_i$ variables. In the second stage, derivatives are calculated by propagating in reverse, from the outputs to the inputs.\n",
    "\n",
    "The most important property of reverse-mode differentiation is that it is **cheaper than forward-mode differentiation for functions with a high number of input variables**. For a function of the form $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}$, only one application of the reverse mode is sufficient to compute the full gradient of the function $\\nabla f = \\big( \\frac{\\partial y}{\\partial x_1}, \\dots ,\\frac{\\partial y}{\\partial x_n} \\big)$. This is the case of deep learning, where the number of input variables is very high. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f0I-yLHWKxB6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "> As we have seen, AD relies on the fact that all numerical computations\n",
    "are ultimately compositions of a finite set of elementary operations for which derivatives are known. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0jadWz_IKxB7",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autograd\n",
    "\n",
    "Autograd is a Python module (with only one function) that implements automatic differentiation.\n",
    "\n",
    "Autograd can automatically differentiate Python and Numpy code:\n",
    "\n",
    "+ It can handle most of Python’s features, including loops, if statements, recursion and closures.\n",
    "+ Autograd allows you to compute gradients of many types of data structures (Any nested combination of lists, tuples, arrays, or dicts).\n",
    "+ It can also compute higher-order derivatives.\n",
    "+ Uses reverse-mode differentiation (backpropagation) so it can efficiently take gradients of scalar-valued functions with respect to array-valued or vector-valued arguments.\n",
    "+ You can easily implement your custom gradients (good for speed, numerical stability, non-compliant code, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Eao5AQjiKxB8",
    "outputId": "8a36acaa-f380-45f6-b586-4490c4dbf8db",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autograd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-6124629ae8b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mautograd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'autograd'"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "x1 = np.array([2, 5], dtype=float)\n",
    "x2 = np.array([5, 2], dtype=float)\n",
    "\n",
    "def test(x):\n",
    "    if x[0]>3:\n",
    "        return np.log(x[0]) + x[0]*x[1] - np.sin(x[1])\n",
    "    else:\n",
    "        return np.log(x[0]) + x[0]*x[1] + np.sin(x[1])\n",
    "      \n",
    "grad_test = grad(test)\n",
    "\n",
    "\n",
    "print(\"({:.2f},{:.2f})\".format(grad_test(x1)[0],grad_test(x1)[1]))\n",
    "print(\"({:.2f},{:.2f})\".format(grad_test(x2)[0],grad_test(x2)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhLa7xHMKxCA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The ``grad`` function:\n",
    "\n",
    "````\n",
    "grad(fun, argnum=0, *nary_op_args, **nary_op_kwargs)\n",
    "\n",
    "Returns a function which computes the gradient of `fun` with respect to positional argument number `argnum`. The returned function takes the same arguments as `fun`, but returns the gradient instead. The function `fun` should be scalar-valued. The gradient has the same type as the argument.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QvIOvBsiKxCC",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then, a simple (there is no bias term) logistic regression model for $n$-dimensional data like this\n",
    "\n",
    "$$  f(x) = \\frac{1}{1 + \\exp^{-(\\mathbf{w}^T \\mathbf{x})}} $$\n",
    "\n",
    "can be implemented in this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autograd'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-7df89daf8ef8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mautograd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'autograd'"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def logistic_predictions(weights, inputs):\n",
    "    return sigmoid(np.dot(inputs, weights))\n",
    "\n",
    "def training_loss(weights, inputs, targets):\n",
    "    preds = logistic_predictions(weights, inputs)\n",
    "    loss = preds * targets + (1 - preds) * (1 - targets)\n",
    "    return -np.sum(np.log(loss))\n",
    "\n",
    "def optimize(inputs, targets, training_loss):\n",
    "    # Optimize weights using gradient descent.\n",
    "    gradient_loss = grad(training_loss)\n",
    "    weights = np.zeros(inputs.shape[1])\n",
    "    print(\"Initial loss:\", training_loss(weights, inputs, targets))\n",
    "    for i in range(100):\n",
    "        weights -= gradient_loss(weights, inputs, targets) * 0.01\n",
    "    print(\"Final loss:\", training_loss(weights, inputs, targets))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-92cec9c6dd04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Weights:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optimize' is not defined"
     ]
    }
   ],
   "source": [
    "# Build a toy dataset with 3d data\n",
    "inputs = np.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "targets = np.array([True, True, False, True])\n",
    "\n",
    "weights = optimize(inputs, targets, training_loss)\n",
    "print(\"Weights:\", weights)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "include_colab_link": true,
   "name": "2. Automatic Differentiation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
